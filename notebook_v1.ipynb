{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f59d6e",
      "metadata": {
        "id": "95f59d6e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2053e67d-6ba2-44cd-c393-e2afc9c98ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip --quiet install datasets transformers soundfile librosa evaluate jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa798782",
      "metadata": {
        "id": "aa798782"
      },
      "source": [
        "## PARTE 1: IMPORT e CARICAMENTO del dataset LibriSpeech"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, Features, Value, Audio, IterableDataset\n",
        "import datasets\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.nn import CTCLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from jiwer import wer, cer\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "from pprint import pprint\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch import amp\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "LAo0mLbqUfyE"
      },
      "id": "LAo0mLbqUfyE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0645b768",
      "metadata": {
        "id": "0645b768"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/data\"\n",
        "\n",
        "train_dataset = LIBRISPEECH(root=data_dir, url=\"train-clean-100\", download=True)\n",
        "eval_dataset = LIBRISPEECH(root=data_dir, url=\"dev-clean\", download=True)\n",
        "test_dataset = LIBRISPEECH(root=data_dir, url=\"test-clean\", download=True)\n",
        "print(\"Caricamento dataset LibriSpeech avvenuto!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_examples(data_path):\n",
        "    examples = []\n",
        "    for root, _, files in os.walk(data_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".trans.txt\"):\n",
        "                with open(os.path.join(root, file), \"r\", encoding=\"utf-8\") as f:\n",
        "                    for line in f:\n",
        "                        parts = line.strip().split(\" \", 1)\n",
        "                        if len(parts) < 2:\n",
        "                            continue\n",
        "                        file_id, text = parts\n",
        "                        audio_path = os.path.join(root, file_id + \".flac\")\n",
        "                        if os.path.exists(audio_path):\n",
        "                            examples.append({\n",
        "                                \"id\": file_id,\n",
        "                                \"audio\": audio_path,\n",
        "                                \"text\": text,\n",
        "                            })\n",
        "    return examples\n",
        "\n",
        "train_examples = generate_examples(\"/content/data/LibriSpeech/train-clean-100\")\n",
        "dev_examples = generate_examples(\"/content/data/LibriSpeech/dev-clean\")\n",
        "test_examples = generate_examples(\"/content/data/LibriSpeech/test-clean\")\n",
        "\n",
        "features = Features({\n",
        "    \"id\": Value(\"string\"),\n",
        "    \"audio\": Audio(sampling_rate=16000),\n",
        "    \"text\": Value(\"string\"),\n",
        "})\n",
        "\n",
        "train_dataset = Dataset.from_list(train_examples).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "dev_dataset = Dataset.from_list(dev_examples).cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "test_dataset = Dataset.from_list(test_examples).cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "sQC_mZTijDUk"
      },
      "id": "sQC_mZTijDUk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFOwKiv3OW6U"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU disponibile: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU non disponibile, usando CPU\")\n",
        "\n",
        "print(f\"Device utilizzato: {device}\")"
      ],
      "id": "zFOwKiv3OW6U"
    },
    {
      "cell_type": "markdown",
      "id": "641dbcdc",
      "metadata": {
        "id": "641dbcdc"
      },
      "source": [
        "## PARTE 2: VOCABOLARIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2013f34c",
      "metadata": {
        "id": "2013f34c"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    text = text.upper()\n",
        "    text = re.sub(r\"[^A-Z ]+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def build_vocab_dict(dataset):\n",
        "    normalized_texts = [normalize_text(t) for t in dataset[\"text\"]]\n",
        "\n",
        "    all_text = \" \".join(normalized_texts)\n",
        "    unique_chars = sorted(set(all_text) - {\" \"})\n",
        "    unique_chars.append(\"|\")\n",
        "\n",
        "    vocab_dict = {c: i for i, c in enumerate(unique_chars)}\n",
        "\n",
        "    vocab_dict[\"<blank>\"] = len(vocab_dict)\n",
        "    vocab_dict[\"<pad>\"] = len(vocab_dict)\n",
        "    vocab_dict[\"<unk>\"] = len(vocab_dict)\n",
        "\n",
        "    return vocab_dict\n",
        "\n",
        "vocab_dict = build_vocab_dict(train_dataset)\n",
        "print(vocab_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fc418ed",
      "metadata": {
        "id": "1fc418ed"
      },
      "source": [
        "## PARTE 3: PRE_PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3da846c",
      "metadata": {
        "id": "e3da846c"
      },
      "outputs": [],
      "source": [
        "class SimpleProcessor:\n",
        "    def __init__(self, vocab_dict, target_sampling_rate=16000, augment=True):\n",
        "        self.vocab = vocab_dict\n",
        "        self.target_sr = target_sampling_rate\n",
        "        self.augment = augment\n",
        "        self._resamplers = {}\n",
        "\n",
        "    def get_resampler(self, orig_sr):\n",
        "        if orig_sr == self.target_sr:\n",
        "            return None\n",
        "        if orig_sr not in self._resamplers:\n",
        "            self._resamplers[orig_sr] = torchaudio.transforms.Resample(orig_sr, self.target_sr)\n",
        "        return self._resamplers[orig_sr]\n",
        "\n",
        "    def preprocess_audio(self, audio_array, orig_sr):\n",
        "        audio = torch.tensor(audio_array, dtype=torch.float32)\n",
        "        resampler = self.get_resampler(orig_sr)\n",
        "        if resampler:\n",
        "            audio = resampler(audio)\n",
        "        return (audio - audio.mean()) / (audio.std() + 1e-5)\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        text = text.replace(\" \", \"|\")\n",
        "        return [self.vocab.get(c, self.vocab[\"<unk>\"]) for c in text]\n",
        "\n",
        "    def __call__(self, audio, sampling_rate, text=None):\n",
        "        inp = self.preprocess_audio(audio, sampling_rate)\n",
        "        out = {\"input_values\": inp}\n",
        "\n",
        "        if text is not None:\n",
        "            out[\"labels\"] = torch.tensor(self.tokenize_text(text), dtype=torch.long)\n",
        "        return out\n",
        "\n",
        "processor = SimpleProcessor(vocab_dict)\n",
        "\n",
        "def preprocess(batch):\n",
        "    audio = batch[\"audio\"][\"array\"]\n",
        "    sr = batch[\"audio\"][\"sampling_rate\"]\n",
        "    text = batch[\"text\"]\n",
        "\n",
        "    processed = processor(audio, sampling_rate=sr, text=text)\n",
        "    return {\n",
        "        \"input_values\": processed[\"input_values\"],\n",
        "        \"labels\": processed[\"labels\"]\n",
        "    }\n",
        "\n",
        "print(\"Avvio preprocessing ottimizzato..\")\n",
        "\n",
        "train_dataset_processed = train_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    num_proc=1,\n",
        ")\n",
        "\n",
        "eval_dataset_processed = dev_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=dev_dataset.column_names,\n",
        "    num_proc=1,\n",
        ")\n",
        "\n",
        "test_dataset_processed = test_dataset.map(\n",
        "    preprocess,\n",
        "    remove_columns=test_dataset.column_names,\n",
        "    num_proc=1,\n",
        ")\n",
        "\n",
        "print(\"Fine preprocessing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Esempio preprocessing"
      ],
      "metadata": {
        "id": "dTM62mh1cmjY"
      },
      "id": "dTM62mh1cmjY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abcaf38c",
      "metadata": {
        "id": "abcaf38c"
      },
      "outputs": [],
      "source": [
        "#esempio per mostrare il preProcessing\n",
        "print(train_dataset_processed[0].keys())\n",
        "print(train_dataset_processed[0][\"input_values\"][:20])\n",
        "print(train_dataset_processed[0][\"labels\"][:20])\n",
        "\n",
        "print(eval_dataset_processed[0][\"input_values\"][:20])\n",
        "print(eval_dataset_processed[0][\"labels\"][:20])\n",
        "\n",
        "print(test_dataset_processed[0][\"input_values\"][:20])\n",
        "print(test_dataset_processed[0][\"labels\"][:20])\n",
        "\n",
        "# In base all'output accade che:\n",
        "\n",
        "# vocabolario: {\"'\": 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14,\n",
        "#               'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, '|': 27, '<pad>': 28,\n",
        "#               '<unk>': 29}\n",
        "\n",
        "# primi 20 caratteri del primo elemento: H A D   L A I D   B E F O R E\n",
        "# diventano: [H, A, D, '|', L, A, I, D, '|', B, E, F, O, R, E] -> [8, 1, 4, 27, 12, 1, 9, 4, 27, 2, 5, 6, 15, 18, 5]\n",
        "# label: [8, 1, 4, 27, 12, 1, 9, 4, 27, 2, 5, 6, 15, 18, 5, 27, ...]\n",
        "\n",
        "#lo stesso per l'eval e test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0feb9d",
      "metadata": {
        "id": "ec0feb9d"
      },
      "source": [
        "## PARTE 4: MODELLO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9c2396",
      "metadata": {
        "id": "ad9c2396"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return x\n",
        "\n",
        "class SimpleASRModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(SimpleASRModel, self).__init__()\n",
        "\n",
        "        # 1. Trasformazione audio -> log-Mel spectrogram\n",
        "        self.melspec = T.MelSpectrogram(\n",
        "            sample_rate=16000,\n",
        "            n_fft=512,\n",
        "            hop_length=160,\n",
        "            n_mels=80\n",
        "        )\n",
        "        self.log_transform = lambda x: torch.log(x + 1e-5)\n",
        "\n",
        "        # 2. CNN per feature extraction (2 layer)\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(80, 256, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(256, 256, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # 3. Positional Encoding\n",
        "        self.positional_encoding = PositionalEncoding(d_model=256)\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=256,\n",
        "                nhead=4,\n",
        "                dim_feedforward=512,\n",
        "                batch_first=True\n",
        "            ),\n",
        "            num_layers=3\n",
        "        )\n",
        "        self.classifier = nn.Linear(256, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.melspec(x)\n",
        "        x = self.log_transform(x)\n",
        "        x = self.cnn(x)\n",
        "        x = x.permute(2, 0, 1)\n",
        "        x = self.positional_encoding(x)\n",
        "        x = self.transformer(x)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b713630f",
      "metadata": {
        "id": "b713630f"
      },
      "source": [
        "## PARTE 5: COLLATE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "252680cf",
      "metadata": {
        "id": "252680cf"
      },
      "outputs": [],
      "source": [
        "def collate(batch):\n",
        "\n",
        "    inputs = [torch.tensor(item[\"input_values\"]) for item in batch]\n",
        "    targets = [torch.tensor(item[\"labels\"]) for item in batch]\n",
        "\n",
        "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
        "\n",
        "    input_lengths = torch.tensor([len(i) for i in inputs])\n",
        "\n",
        "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=vocab_dict[\"<pad>\"])\n",
        "\n",
        "    target_lengths = torch.tensor([len(t) for t in targets])\n",
        "\n",
        "    return {\n",
        "        \"input_values\": inputs_padded,\n",
        "        \"labels\": targets_padded,\n",
        "        \"input_lengths\": input_lengths,\n",
        "        \"label_lengths\": target_lengths\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aec0fde",
      "metadata": {
        "id": "6aec0fde"
      },
      "source": [
        "## PARTE 5: TRAIN LOOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8380a31b",
      "metadata": {
        "id": "8380a31b"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SimpleASRModel(vocab_size=len(vocab_dict)).to(device)\n",
        "model = torch.compile(model)\n",
        "print(\"Model on:\", next(model.parameters()).device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "criterion = CTCLoss(blank=vocab_dict[\"<blank>\"], zero_infinity=True)\n",
        "scaler = amp.GradScaler(device=\"cuda\")\n",
        "\n",
        "train_batch_size = 1024\n",
        "eval_batch_size = 1024\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset_processed,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate,\n",
        "    pin_memory=True,\n",
        "    num_workers=4\n",
        ")\n",
        "dev_loader = DataLoader(\n",
        "    eval_dataset_processed,\n",
        "    batch_size=eval_batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate,\n",
        "    pin_memory=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "print(\"Numero di batch per epoca:\", len(train_loader))\n",
        "\n",
        "inv_vocab = {v: k for k, v in vocab_dict.items()}\n",
        "def greedy_decode(log_probs, vocab_dict):\n",
        "    pred_ids = torch.argmax(log_probs, dim=-1)\n",
        "    pred_texts = []\n",
        "\n",
        "    for b in range(pred_ids.shape[1]):\n",
        "        prev = None\n",
        "        sentence = []\n",
        "        for t in range(pred_ids.shape[0]):\n",
        "            idx = pred_ids[t, b].item()\n",
        "            if idx != prev and idx != vocab_dict[\"<blank>\"] and idx != vocab_dict[\"<pad>\"]:\n",
        "                if idx in inv_vocab:\n",
        "                    sentence.append(inv_vocab[idx])\n",
        "                prev = idx\n",
        "        pred_texts.append(\"\".join(sentence))\n",
        "    return pred_texts\n",
        "\n",
        "def compute_output_lengths(input_lengths, num_layers=2, stride=2):\n",
        "    for _ in range(num_layers):\n",
        "        input_lengths = (input_lengths + 1) // stride\n",
        "    return input_lengths\n",
        "\n",
        "def monitor_gpu():\n",
        "  if torch.cuda.is_available():\n",
        "      allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "      reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "      print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
        "\n",
        "patience = 10\n",
        "best_loss = float('inf')\n",
        "no_improve_epochs = 0\n",
        "monitor_gpu()\n",
        "\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs = batch[\"input_values\"].to(device)\n",
        "        targets = batch[\"labels\"].to(device)\n",
        "        input_lengths = batch[\"input_lengths\"].to(device)\n",
        "        target_lengths = batch[\"label_lengths\"].to(device)\n",
        "\n",
        "        mel_frame_length = ((input_lengths - 400) // 160) + 1\n",
        "        adjusted_input_lengths = compute_output_lengths(mel_frame_length).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast(\"cuda\"):\n",
        "          outputs = model(inputs)\n",
        "          log_probs = F.log_softmax(outputs, dim=-1)\n",
        "          flattened_targets = torch.cat([targets[i, :target_lengths[i]] for i in range(targets.size(0))])\n",
        "          loss = criterion(log_probs, flattened_targets, adjusted_input_lengths, target_lengths)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    duration = time.time() - start_time\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Durata: {duration:.2f}s\")\n",
        "\n",
        "     # ======= VALUTAZIONE SU DEV ========\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "      model.eval()\n",
        "      all_preds, all_targets = [], []\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for batch in dev_loader:\n",
        "              inputs = batch[\"input_values\"].to(device)\n",
        "              targets = batch[\"labels\"]\n",
        "              target_lengths = batch[\"label_lengths\"]\n",
        "\n",
        "              outputs = model(inputs)\n",
        "              log_probs = F.log_softmax(outputs, dim=-1)\n",
        "\n",
        "              pred_texts = greedy_decode(log_probs.cpu(), vocab_dict)\n",
        "\n",
        "              inv_vocab = {v: k for k, v in vocab_dict.items()}\n",
        "              for i, length in enumerate(target_lengths):\n",
        "                  target_ids = targets[i][:length].tolist()\n",
        "                  target_text = \"\".join([inv_vocab[id] for id in target_ids])\n",
        "                  target_text = target_text.replace(\"|\", \" \")\n",
        "                  all_targets.append(target_text)\n",
        "\n",
        "              all_preds.extend(pred_texts)\n",
        "\n",
        "      wer_score = wer(all_targets, all_preds)\n",
        "      cer_score = cer(all_targets, all_preds)\n",
        "      print(f\"Epoch {epoch+1} - WER: {wer_score:.4f}, CER: {cer_score:.4f}\")\n",
        "\n",
        "      print(\"\\nEsempi di predizioni:\")\n",
        "      for i in range(min(3, len(all_preds))):\n",
        "          print(f\"Ref: {all_targets[i]}\")\n",
        "          print(f\"Pred: {all_preds[i]}\")\n",
        "      monitor_gpu()\n",
        "\n",
        "    scheduler.step(avg_loss)\n",
        "\n",
        "    # ======= EARLY STOPPING ========\n",
        "\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        no_improve_epochs = 0\n",
        "        print(f\"Nuovo best loss: {best_loss:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "    else:\n",
        "        no_improve_epochs += 1\n",
        "        print(f\"Nessun miglioramento della loss ({no_improve_epochs}/{patience})\")\n",
        "        print(\"-\" * 50)\n",
        "    if no_improve_epochs >= patience:\n",
        "        print(\"Early stopping!\")\n",
        "        print(\"-\" * 50)\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}