{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f59d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets transformers soundfile librosa evaluate jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa798782",
   "metadata": {},
   "source": [
    "# PARTE 1: IMPORT e CARICAMENTO del dataset LibriSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645b768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento dataset LibriSpeech avvenuto!\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CTCLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from jiwer import wer, cer\n",
    "import time\n",
    "from torch.utils.data import Subset\n",
    "import random\n",
    "\n",
    "train_dataset = load_dataset(\"./local_dataset_loader\", name=\"local_dataset_loader\" ,split=\"train\", data_dir=\"./libriSpeech_data/LibriSpeech\", trust_remote_code=True)\n",
    "eval_dataset = load_dataset(\"./local_dataset_loader\", name=\"local_dataset_loader\", split=\"validation\", data_dir=\"./libriSpeech_data/LibriSpeech\", trust_remote_code=True)\n",
    "test_dataset = load_dataset(\"./local_dataset_loader\", name=\"local_dataset_loader\",split=\"test\", data_dir=\"./libriSpeech_data/LibriSpeech\", trust_remote_code=True)\n",
    "print(\"Caricamento dataset LibriSpeech avvenuto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dbcdc",
   "metadata": {},
   "source": [
    "# PARTE 2: VOCABOLARIO\n",
    "\n",
    "Serve a mappare ogni carattere (o token) a un indice numerico. Nel nostro caso, il modello trascrive caratteri (non parole), quindi il vocabolario è un char-level mapping \n",
    "{\"'\": 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, '|': 27, '<blank>': 28, '<pad>': 29, '<unk>': 30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013f34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'\": 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, '|': 27, '<blank>': 28, '<pad>': 29, '<unk>': 30}\n"
     ]
    }
   ],
   "source": [
    "def build_vocab_dict(dataset):\n",
    "    all_text = \" \".join(dataset[\"text\"])\n",
    "    unique_chars = sorted(set(all_text) - {\" \"})\n",
    "    unique_chars.append(\"|\")\n",
    "\n",
    "    vocab_dict = {c: i for i, c in enumerate(unique_chars)}\n",
    "\n",
    "    vocab_dict[\"<blank>\"] = len(vocab_dict)\n",
    "    vocab_dict[\"<pad>\"] = len(vocab_dict) \n",
    "    vocab_dict[\"<unk>\"] = len(vocab_dict)\n",
    "\n",
    "    return vocab_dict\n",
    "\n",
    "vocab_dict = build_vocab_dict(train_dataset)\n",
    "print(vocab_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc418ed",
   "metadata": {},
   "source": [
    "# PARTE 3: PRE_PROCESSING\n",
    "\n",
    "# Conversione dell’audio in tensori numerici\n",
    "I modelli deep learning non possono lavorare direttamente con file audio o oggetti “audio”.\n",
    "Il preprocessing estrae l’audio in forma di tensore float32, lo normalizza, e (se necessario) lo ricampiona alla frequenza standard (16kHz).\n",
    "\n",
    "# Tokenizzazione del testo\n",
    "Trasforma la trascrizione da testo in una sequenza di interi (labels), dove ogni carattere è mappato a un ID del vocabolario.\n",
    "È essenziale per usare la CTC Loss, che lavora su sequenze di ID e non stringhe di testo.\n",
    "\n",
    "# Dopo questo preprocessing abbiamo: \n",
    "\"input_values\" → tensore audio normalizzato, pronto per essere passato a CNN o Transformer.\n",
    "\"labels\" → lista di ID dei caratteri della trascrizione, per calcolare la CTC loss.\n",
    "\n",
    "# Fare il mapping su train, validation e test ha senso: \n",
    "train: per addestrare il modello con input numerici\n",
    "validation: per monitorare la performance durante il training\n",
    "test: per valutare il modello finalizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avvio preprocessing ottimizzato..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1eb455b3a894bdbb563bac8662e549c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/28539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9114f2b63cc441549c3b4f11afc9303a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75ab21c3a9f4d528d02206df3a20185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine preprocessing!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SimpleProcessor:\n",
    "    def __init__(self, vocab_dict, target_sampling_rate=16000, augment=True):\n",
    "        self.vocab = vocab_dict\n",
    "        self.target_sr = target_sampling_rate\n",
    "        self.augment = augment\n",
    "\n",
    "    def preprocess_audio(self, audio_array, orig_sr):\n",
    "        if orig_sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_sr, self.target_sr)\n",
    "            audio_array = resampler(torch.tensor(audio_array).float())\n",
    "        else:\n",
    "            audio_array = torch.tensor(audio_array).float()\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                volume_factor = random.uniform(0.8, 1.2)\n",
    "                audio_array = audio_array * volume_factor\n",
    "\n",
    "            if random.random() < 0.3:\n",
    "                noise_factor = random.uniform(0.001, 0.01)\n",
    "                noise = torch.randn_like(audio_array) * noise_factor\n",
    "                audio_array = audio_array + noise\n",
    "\n",
    "        audio_array = (audio_array - audio_array.mean()) / (audio_array.std() + 1e-5)\n",
    "\n",
    "        return audio_array\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        text = text.replace(\" \", \"|\")\n",
    "        return [self.vocab.get(c, self.vocab[\"<unk>\"]) for c in text]\n",
    "\n",
    "    def __call__(self, audio, sampling_rate, text=None):\n",
    "        inputs = self.preprocess_audio(audio, sampling_rate)\n",
    "\n",
    "        result = {\"input_values\": inputs}\n",
    "        if text is not None:\n",
    "            result[\"labels\"] = torch.tensor(self.tokenize_text(text), dtype=torch.long)\n",
    "        return result\n",
    "\n",
    "processor = SimpleProcessor(vocab_dict)\n",
    "\n",
    "def preprocess(batch):\n",
    "    audio = batch[\"audio\"][\"array\"]\n",
    "    sr = batch[\"audio\"][\"sampling_rate\"]\n",
    "    text = batch[\"text\"]\n",
    "\n",
    "    processed = processor(audio, sampling_rate=sr, text=text)\n",
    "    return {\n",
    "        \"input_values\": processed[\"input_values\"],\n",
    "        \"labels\": processed[\"labels\"]\n",
    "    }\n",
    "    \n",
    "print(\"Avvio preprocessing ottimizzato..\")\n",
    "\n",
    "train_dataset_processed = train_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "eval_dataset_processed = eval_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "test_dataset_processed = test_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=test_dataset.column_names,\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "print(\"Fine preprocessing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcaf38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_values', 'labels'])\n",
      "[-0.15206488966941833, -0.17493396997451782, 0.1320481300354004, 0.003173402277752757, -0.020163103938102722, 0.029104067012667656, -0.07104626297950745, 0.08140091598033905, -0.09795332700014114, 0.039900705218315125, -0.07659906148910522, 0.14178574085235596, -0.014021406881511211, 0.0908619835972786, -0.2531350553035736, 0.013963003642857075, -0.2295738309621811, 0.06235656514763832, 0.053652241826057434, 0.14166110754013062]\n",
      "[8, 1, 4, 27, 12, 1, 9, 4, 27, 2, 5, 6, 15, 18, 5, 27, 8, 5, 18, 27]\n",
      "[0.0007413655985146761, 0.0014564606826752424, 0.0021715557668358088, 0.0007413655985146761, 0.0007413655985146761, 0.0021715557668358088, 0.0021715557668358088, 0.0014564606826752424, 0.0014564606826752424, 0.0014564606826752424, 0.0014564606826752424, 0.0014564606826752424, 0.0014564606826752424, 0.0014564606826752424, 0.002886650850996375, 0.0021715557668358088, 0.0021715557668358088, 0.0021715557668358088, 0.0014564606826752424, 0.0014564606826752424]\n",
      "[19, 8, 15, 18, 20, 12, 25, 27, 1, 6, 20, 5, 18, 27, 16, 1, 19, 19, 9, 14]\n",
      "[-0.045244451612234116, -0.006274897139519453, 0.19141703844070435, 0.005515142343938351, -0.022698577493429184, 0.147803395986557, 0.05081850662827492, 0.1424807459115982, 0.07080940157175064, 0.1592939794063568, 0.05007239803671837, 0.23300844430923462, 0.06642192602157593, 0.1790512651205063, -0.059537164866924286, 0.10066285729408264, -0.01618179865181446, 0.28668490052223206, 0.1878446787595749, 0.23653635382652283]\n",
      "[25, 15, 21, 14, 7, 27, 6, 9, 20, 26, 15, 15, 20, 8, 27, 8, 1, 4, 27, 2]\n"
     ]
    }
   ],
   "source": [
    "#esempio per mostrare il preProcessing\n",
    "print(train_dataset_processed[0].keys())\n",
    "print(train_dataset_processed[0][\"input_values\"][:20])\n",
    "print(train_dataset_processed[0][\"labels\"][:20])\n",
    "\n",
    "print(eval_dataset_processed[0][\"input_values\"][:20])\n",
    "print(eval_dataset_processed[0][\"labels\"][:20])\n",
    "\n",
    "print(test_dataset_processed[0][\"input_values\"][:20])\n",
    "print(test_dataset_processed[0][\"labels\"][:20])\n",
    "\n",
    "# In base all'output accade che:\n",
    "\n",
    "# vocabolario: {\"'\": 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, \n",
    "#               'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, '|': 27, '<pad>': 28, \n",
    "#               '<unk>': 29}\n",
    "\n",
    "# primi 20 caratteri del primo elemento: H A D   L A I D   B E F O R E\n",
    "# diventano: [H, A, D, '|', L, A, I, D, '|', B, E, F, O, R, E] -> [8, 1, 4, 27, 12, 1, 9, 4, 27, 2, 5, 6, 15, 18, 5] \n",
    "# label: [8, 1, 4, 27, 12, 1, 9, 4, 27, 2, 5, 6, 15, 18, 5, 27, ...]\n",
    "\n",
    "#lo stesso per l'eval e test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0feb9d",
   "metadata": {},
   "source": [
    "# PARTE 4: MODELLO\n",
    "\n",
    "1. Prende l’audio grezzo (onda sonora) e lo trasforma in un’immagine chiamata spettrogramma Mel-log - una rappresentazione che mostra come l’energia del suono cambia nel tempo e nelle frequenze, più facile da interpretare per la rete.\n",
    "\n",
    "2. Passa questo spettrogramma a una CNN (convoluzione) che estrae caratteristiche importanti e riduce la lunghezza della sequenza nel tempo riassumendo l’informazione utile.\n",
    " \n",
    "3. Le caratteristiche ottenute vengono date a un Transformer, che capisce le dipendenze temporali più complesse e le relazioni  tra le parti della sequenza audio\n",
    "\n",
    "4. Infine, un layer lineare trasforma queste informazioni in punteggi (logits) per ogni possibile carattere del vocabolario, cioè decide quali lettere o simboli sono più probabili in ogni momento.\n",
    "Questi punteggi vengono poi usati per calcolare la perdita (loss) con la CTC e per generare il testo trascritto.\n",
    "\n",
    "IN GENERALE: è utile perché trasforma il suono grezzo, che è difficile da interpretare direttamente, in una rappresentazione più ricca e strutturata (lo spettrogramma Mel-log) che cattura le caratteristiche importanti del parlato.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleASRModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SimpleASRModel, self).__init__()\n",
    "\n",
    "        # 1. Trasformazione audio -> log-Mel spectrogram\n",
    "        self.melspec = T.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_fft=400,\n",
    "            hop_length=160,\n",
    "            n_mels=128\n",
    "        )\n",
    "        self.log_transform = lambda x: torch.log(x + 1e-5)\n",
    "\n",
    "        # 2. CNN per feature extraction (2 layer)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(256, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 3. Transformer Encoder\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=256,\n",
    "                nhead=4,\n",
    "                dim_feedforward=512\n",
    "            ),\n",
    "            num_layers=3\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(256, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, audio_len) - waveform normalizzato\n",
    "        \"\"\"\n",
    "        x = self.melspec(x)\n",
    "        x = self.log_transform(x)\n",
    "\n",
    "        x = self.cnn(x)\n",
    "        x = x.permute(2, 0, 1)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b713630f",
   "metadata": {},
   "source": [
    "# PARTE 5: COLLATE\n",
    "\n",
    "Quando si usa un DataLoader con un batch di dati di lunghezze variabili (come onde audio o sequenze di caratteri), serve una funzione collate per allineare tutto correttamente in un batch tensoriale. \n",
    "\n",
    "Quindi questo prepara il batch per il training, gestendo sequenze variabili.\n",
    "\n",
    "cosa succede:\n",
    "1. vengono estratte delle sequenze audio (input_values) e testo (labels):\n",
    "2. viene applicato del padding alle sequenze audio (utile nel caso della eval)\n",
    "3. vengono calcolate le lunghezze originali delle sequenze audio prima del padding\n",
    "4. viene applicato il padding alle label testuali\n",
    "5. vengono calcolate le lunghezze reali delle label\n",
    "6. viene ritornato tutto in un unico dizionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252680cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    \n",
    "    inputs = [torch.tensor(item[\"input_values\"]) for item in batch]\n",
    "    targets = [torch.tensor(item[\"labels\"]) for item in batch]  \n",
    "    \n",
    "    inputs_padded = nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
    "    \n",
    "    input_lengths = torch.tensor([len(i) for i in inputs])\n",
    "\n",
    "    targets_padded = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=vocab_dict[\"<pad>\"])\n",
    "\n",
    "    target_lengths = torch.tensor([len(t) for t in targets])\n",
    "        \n",
    "    return {\n",
    "        \"input_values\": inputs_padded, \n",
    "        \"labels\": targets_padded, \n",
    "        \"input_lengths\": input_lengths, \n",
    "        \"label_lengths\": target_lengths\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec0fde",
   "metadata": {},
   "source": [
    "# PARTE 5: TRAIN LOOP\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8380a31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di batch per epoca: 250\n",
      "Epoch 1, Loss: 2.8777, Durata: 91.18s\n",
      "Epoch 1 - WER: 1.0000, CER: 0.9497\n",
      "\n",
      "Esempi di predizioni:\n",
      "Ref: SHORTLY AFTER PASSING ONE OF THESE CHAPELS WE CAME SUDDENLY UPON A VILLAGE WHICH STARTED UP OUT OF THE MIST AND I WAS ALARMED LEST I SHOULD BE MADE AN OBJECT OF CURIOSITY OR DISLIKE\n",
      "Pred: |A|A|T|A|IN|S|AN|A|A|A|E|E|A|R|A|E|N|A|R|\n",
      "Ref: MY GUIDES HOWEVER WERE WELL KNOWN AND THE NATURAL POLITENESS OF THE PEOPLE PREVENTED THEM FROM PUTTING ME TO ANY INCONVENIENCE BUT THEY COULD NOT HELP EYEING ME NOR I THEM\n",
      "Pred: |A|A|AT|R|R|R|AE|N|A|A|A|R|E|T|R|RE|R\n",
      "Ref: THE STREETS WERE NARROW AND UNPAVED BUT VERY FAIRLY CLEAN\n",
      "Pred: I|AR|R|A|\n",
      "--------------------------------------------------\n",
      "Epoch 2, Loss: 2.4514, Durata: 112.85s\n",
      "Epoch 2 - WER: 1.0000, CER: 0.7488\n",
      "\n",
      "Esempi di predizioni:\n",
      "Ref: SHORTLY AFTER PASSING ONE OF THESE CHAPELS WE CAME SUDDENLY UPON A VILLAGE WHICH STARTED UP OUT OF THE MIST AND I WAS ALARMED LEST I SHOULD BE MADE AN OBJECT OF CURIOSITY OR DISLIKE\n",
      "Pred: |O|R|T|TE|AT|IE|IE|TAT|OLS|E|E|EAD|E|ON|E|E|OE|TA|E|AE|NE|NE|EN|O|WNE|T|AT|TAE|E|E|E|T|IE|E|A|T|TE|E|WR|E|FO\n",
      "Ref: MY GUIDES HOWEVER WERE WELL KNOWN AND THE NATURAL POLITENESS OF THE PEOPLE PREVENTED THEM FROM PUTTING ME TO ANY INCONVENIENCE BUT THEY COULD NOT HELP EYEING ME NOR I THEM\n",
      "Pred: |AIE|IT|AT|E|WOE|W|EOER|AN|E|THO|L|ETOE|E|OL|ER|AN|E|ER|HE|WE|E|E|E|E|AITIE|OT|E|E|E|AR\n",
      "Ref: THE STREETS WERE NARROW AND UNPAVED BUT VERY FAIRLY CLEAN\n",
      "Pred: |IES|E|OE|R|A|N|ET|ERE|AE|A|IE|HE\n",
      "--------------------------------------------------\n",
      "Epoch 3, Loss: 2.2100, Durata: 115.87s\n",
      "Epoch 3 - WER: 1.0000, CER: 0.7404\n",
      "\n",
      "Esempi di predizioni:\n",
      "Ref: SHORTLY AFTER PASSING ONE OF THESE CHAPELS WE CAME SUDDENLY UPON A VILLAGE WHICH STARTED UP OUT OF THE MIST AND I WAS ALARMED LEST I SHOULD BE MADE AN OBJECT OF CURIOSITY OR DISLIKE\n",
      "Pred: |HO|T|TE|HAFHIN|I|E|TOT|OLS|E|HIND|FEAD|E|ON|ED|A|DOE|ADE|AT|ONER|N|I|AN|NO|O|OND|T|LOS|HA|HE|E|MNDE|OT|E|E|HA|ST|HE|E|WOR|O|FOIY\n",
      "Ref: MY GUIDES HOWEVER WERE WELL KNOWN AND THE NATURAL POLITENESS OF THE PEOPLE PREVENTED THEM FROM PUTTING ME TO ANY INCONVENIENCE BUT THEY COULD NOT HELP EYEING ME NOR I THEM\n",
      "Pred: |IN|T|HOATHO|WOE|O|O|R|AN|NACTHO|L|ETOE|HE|OL|OR|AN|E|ER|O|ORE|E|T|EN|I|E|HE|AITOE|OT|EN|E|I|N\n",
      "Ref: THE STREETS WERE NARROW AND UNPAVED BUT VERY FAIRLY CLEAN\n",
      "Pred: HIS|HE|E|RO|A|ON|P|ETDT|AOUD|HA|HAI|LE\n",
      "--------------------------------------------------\n",
      "Epoch 4, Loss: 2.0748, Durata: 113.33s\n",
      "Epoch 4 - WER: 1.0000, CER: 0.7253\n",
      "\n",
      "Esempi di predizioni:\n",
      "Ref: SHORTLY AFTER PASSING ONE OF THESE CHAPELS WE CAME SUDDENLY UPON A VILLAGE WHICH STARTED UP OUT OF THE MIST AND I WAS ALARMED LEST I SHOULD BE MADE AN OBJECT OF CURIOSITY OR DISLIKE\n",
      "Pred: |HO|E|T|TO|HAF|HIN|IE|TAT|POLS|E|HI|FEAD|E|ON|E|E|DOE|TINDED|A|HODE|N|AI|AN|IR|T|OSF|HESHE|E|MIDE|O|GI|OE|TA|OTHADE|ORDO|FI\n",
      "Ref: MY GUIDES HOWEVER WERE WELL KNOWN AND THE NATURAL POLITENESS OF THE PEOPLE PREVENTED THEM FROM PUTTING ME TO ANY INCONVENIENCE BUT THEY COULD NOT HELP EYEING ME NOR I THEM\n",
      "Pred: INDITHOUTHR|O|O|OAD|NACTHO|P|L|I|O|HE|BO|HR|AD|TORE|FO|WRE|TIR|E|E|HE|ATCOE|OT|I|AE|I|N\n",
      "Ref: THE STREETS WERE NARROW AND UNPAVED BUT VERY FAIRLY CLEAN\n",
      "Pred: HIS|HEH|E|RO|ION|PAE|T|BO|HA|HAI|LE\n",
      "--------------------------------------------------\n",
      "Epoch 5, Loss: 1.9776, Durata: 112.10s\n",
      "Epoch 5 - WER: 1.0000, CER: 0.7152\n",
      "\n",
      "Esempi di predizioni:\n",
      "Ref: SHORTLY AFTER PASSING ONE OF THESE CHAPELS WE CAME SUDDENLY UPON A VILLAGE WHICH STARTED UP OUT OF THE MIST AND I WAS ALARMED LEST I SHOULD BE MADE AN OBJECT OF CURIOSITY OR DISLIKE\n",
      "Pred: |HORO|E|AT|TH|HAF|FIN|IES|SOAT|OLS|E|HA|FEAE|E|ON|E|NO|DOE|SIN|E|A|HODO|NE|I|AN|AN|W|NRE|T|OSF|HEAT|HO|E|MINDE|NOT|GOE|O|HERE|OST|H|E|WOR|O|FIR|\n",
      "Ref: MY GUIDES HOWEVER WERE WELL KNOWN AND THE NATURAL POLITENESS OF THE PEOPLE PREVENTED THEM FROM PUTTING ME TO ANY INCONVENIENCE BUT THEY COULD NOT HELP EYEING ME NOR I THEM\n",
      "Pred: |IT|DIT|OTHER|W|O|O|O|W|OD|O|NATHO|L|I|OT|O|E|HE|OL|HOE|DEAN|E|FEO|WRE|E|R|AEN|HN|E|HE|HT|OE|OT|I|ME|N|IE|NAW\n",
      "Ref: THE STREETS WERE NARROW AND UNPAVED BUT VERY FAIRLY CLEAN\n",
      "Pred: HS|HE|RE|ARO|I|ON|PAN|E|TD|BO|HERT|HAE|LE|LH\n",
      "--------------------------------------------------\n",
      "Epoch 6, Loss: 1.9006, Durata: 114.87s\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SimpleASRModel(vocab_size=len(vocab_dict)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# CTC usa il token blank per gestire l'allineamento temporale\n",
    "criterion = CTCLoss(blank=vocab_dict[\"<blank>\"], zero_infinity=True)\n",
    "\n",
    "subset_indices = list(range(1000))\n",
    "small_dataset = Subset(train_dataset_processed, subset_indices)\n",
    "\n",
    "#train_loader = DataLoader(train_dataset_processed, batch_size=4, shuffle=True, collate_fn=collate)\n",
    "\n",
    "train_loader = DataLoader(small_dataset, batch_size=4, shuffle=True, collate_fn=collate)\n",
    "dev_loader = DataLoader(eval_dataset_processed, batch_size=4, shuffle=False, collate_fn=collate)\n",
    "\n",
    "print(\"Numero di batch per epoca:\", len(train_loader))\n",
    "\n",
    "# La funzione greedy_decode(log_probs, vocab_dict) serve per convertire le probabilità logaritmiche (output del modello) \n",
    "# in una trascrizione testuale leggibile, scegliendo per ogni time step il carattere più probabile.\n",
    "def greedy_decode(log_probs, vocab_dict):\n",
    "    inv_vocab = {v: k for k, v in vocab_dict.items()}\n",
    "    pred_ids = torch.argmax(log_probs, dim=-1)\n",
    "    pred_texts = []\n",
    "\n",
    "    for b in range(pred_ids.shape[1]):\n",
    "        prev = None\n",
    "        sentence = []\n",
    "        for t in range(pred_ids.shape[0]):\n",
    "            idx = pred_ids[t, b].item()\n",
    "            if idx != prev and idx != vocab_dict[\"<blank>\"] and idx != vocab_dict[\"<pad>\"]:\n",
    "                if idx in inv_vocab:\n",
    "                    sentence.append(inv_vocab[idx])\n",
    "                prev = idx\n",
    "        pred_texts.append(\"\".join(sentence))\n",
    "    return pred_texts\n",
    "\n",
    "# Con stride=2, ogni layer dimezza approssimativamente la dimensione temporale. Quindi:\n",
    "#    Dopo prima conv: T → T/2\n",
    "#    Dopo seconda conv: T/2 → T/4\n",
    "# Ecco perché compute_output_lengths divide per 2 due volte.\n",
    "def compute_output_lengths(input_lengths, num_layers=2, stride=2):\n",
    "    for _ in range(num_layers):\n",
    "        input_lengths = (input_lengths + 1) // stride\n",
    "    return input_lengths\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = batch[\"input_values\"].to(device)\n",
    "        targets = batch[\"labels\"].to(device)\n",
    "        input_lengths = batch[\"input_lengths\"].to(device)\n",
    "        target_lengths = batch[\"label_lengths\"].to(device)\n",
    "\n",
    "        # Ecco cosa avveine:\n",
    "        # 1. Abbiamo un audio (waveform) lungo input_lengths (in campioni, es. 16000 = 1 secondo).\n",
    "        # 2. Lo trasformiamo in un mel-spectrogramma, cioè una sequenza di “frame” nel tempo.\n",
    "        # 3. Passiamo quei frame nella CNN, che ha 2 layer con stride=2. La lunghezza la calcoliamo con compute\n",
    "        # 4. La CTC Loss ha bisogno di sapere:\n",
    "        #    T: quante previsioni temporali ha il modello (adjusted_input_lengths)\n",
    "        #    U: quanto è lunga la trascrizione (target_lengths)\n",
    "\n",
    "        #Waveform -> Mel-Spectrogram\n",
    "        # mel_frame_length = ((input_len - n_fft) // hop_length) + 1\n",
    "        mel_frame_length = ((input_lengths - 400) // 160) + 1\n",
    "        #La CNN ha 2 layer con stride=2, quindi la dimensione temporale si riduce\n",
    "        # È la vera lunghezza temporale dell’output del modello, cioè quante “previsioni” fa il modello nel tempo per ogni audio nel batch.\n",
    "        adjusted_input_lengths = compute_output_lengths(mel_frame_length).to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        log_probs = nn.functional.log_softmax(outputs, dim=-1)\n",
    "\n",
    "        # Appiattisci le label reali rimuovendo i padding\n",
    "        flattened_targets = torch.cat([targets[i, :target_lengths[i]] for i in range(targets.size(0))])\n",
    "\n",
    "        loss = criterion(log_probs, flattened_targets, adjusted_input_lengths, target_lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()        \n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}, Durata: {duration:.2f}s\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    # ======= VALUTAZIONE SU DEV ========\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            inputs = batch[\"input_values\"].to(device)\n",
    "            targets = batch[\"labels\"]\n",
    "            target_lengths = batch[\"label_lengths\"]\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            log_probs = nn.functional.log_softmax(outputs, dim=-1)\n",
    "\n",
    "            pred_texts = greedy_decode(log_probs.cpu(), vocab_dict)\n",
    "\n",
    "            inv_vocab = {v: k for k, v in vocab_dict.items()}\n",
    "            for i, length in enumerate(target_lengths):\n",
    "                target_ids = targets[i][:length].tolist()\n",
    "                target_text = \"\".join([inv_vocab[id] for id in target_ids])\n",
    "                target_text = target_text.replace(\"|\", \" \")\n",
    "                all_targets.append(target_text)\n",
    "\n",
    "            all_preds.extend(pred_texts)\n",
    "\n",
    "    wer_score = wer(all_targets, all_preds)\n",
    "    cer_score = cer(all_targets, all_preds)\n",
    "    print(f\"Epoch {epoch+1} - WER: {wer_score:.4f}, CER: {cer_score:.4f}\")\n",
    "\n",
    "    print(\"\\nEsempi di predizioni:\")\n",
    "    for i in range(min(3, len(all_preds))):\n",
    "        print(f\"Ref: {all_targets[i]}\")\n",
    "        print(f\"Pred: {all_preds[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
